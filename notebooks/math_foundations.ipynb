{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plasmid LoRA Swarm – Mathematical Foundations\n",
        "\n",
        "This notebook presents the core mathematical framework used in the Plasmid LoRA Swarm project.\n",
        "\n",
        "## 1. Linear Algebra for LoRA\n",
        "- Low-rank decomposition: A ∈ R^{m×r}, B ∈ R^{r×n}, ΔW = B A.\n",
        "- Compositional merging: weighted deltas, SVD-based rank-k re-projection.\n",
        "- Subspace analysis: principal angles, cos^2 overlap, effective rank.\n",
        "\n",
        "## 2. Information Theory\n",
        "- Cross-entropy, KL and JS divergences on logits.\n",
        "- Mutual information across (Agent, Domain) presence; information gain per round.\n",
        "- PID-lite proxies: co-occurrence excess, synergy/redundancy aggregates.\n",
        "\n",
        "## 3. Graph Theory\n",
        "- Topologies: Erdős–Rényi, Watts–Strogatz, Barabási–Albert.\n",
        "- Spectral gap (λ₂) and heuristic diffusion predictor t ~ C log n / λ₂.\n",
        "- Rounds-to-diffuse, coverage/entropy dynamics.\n",
        "\n",
        "## 4. Optimization and Composition\n",
        "- Alternating train–merge schedules; trust-region scaling (‖Δ‖_F ≤ τ).\n",
        "- Fisher-weighted merging (diagonal Fisher proxies).\n",
        "\n",
        "## 5. Security and Alignment\n",
        "- Policy gate: ranks/targets/size/SHA/signatures with quorum.\n",
        "- Behavioral probes: trigger rate, clean accuracy deltas.\n",
        "- Reputation gating and verifiable audit logs (hash chain).\n",
        "\n",
        "## 6. Pointers to Code\n",
        "- See docs/module_links.md for mapping from concepts to modules.\n",
        "\n",
        "> This notebook is a living document; fill in derivations and proofs as experiments evolve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LoRA Weighted Merge and Re-projection Bounds\n",
        "\n",
        "Let base weights be W₀ and adapters produce deltas ΔWᵢ = BᵢAᵢ (rank rᵢ). Weighted composition:\n",
        "\n",
        "\\[ W' = W_0 + \\sum_i w_i \\Delta W_i. \\]\n",
        "\n",
        "Re-projection to rank k chooses ΔW*_k = argmin_{rank≤k} ||ΔW - X||_F = U_k S_k V_k^T (Eckart–Young–Mirsky). Error bound:\n",
        "\n",
        "\\[ ||ΔW - ΔW*_k||_F^2 = \\sum_{j>k} \\sigma_j^2. \\]\n",
        "\n",
        "Trust region scales ΔW by α ≤ τ/||ΔW||_F to enforce ||αΔW||_F ≤ τ.\n",
        "\n",
        "Assumptions for stability: bounded operator norms of Jacobians under small ΔW; approximate additivity when adapter supports are weakly overlapping (principal angles large). See `plora/la_utils.py` for principal angles and effective rank.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mixing-time and Spectral Gap\n",
        "\n",
        "Consider an undirected connected graph with Laplacian L and eigenvalues 0=λ₁ ≤ λ₂ ≤ … ≤ λₙ. For lazy random walks, the mixing time t_mix(ε) satisfies bounds involving λ₂ and conductance Φ:\n",
        "\n",
        "- Cheeger: (1/2)Φ² ≤ λ₂ ≤ 2Φ\n",
        "- t_mix(ε) ≤ (1/λ₂) log(1/ε π_min)\n",
        "\n",
        "Heuristic for push–pull gossip rounds: \\( t_\\text{diffuse} \\approx C \\frac{\\log n}{\\lambda_2} \\). We calibrate C per topology in `scripts/validate_bounds.py`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Information Flow: Inter-model MI and Transfer Entropy\n",
        "\n",
        "We proxy I(M;Y) via Jensen–Shannon divergence of next-token distributions from two models, averaged over tokens. This captures how informative model choice is about emitted symbols.\n",
        "\n",
        "Transfer entropy across rounds is approximated by the increase in R² when adding lagged source series to a regression predicting the target series. See `plora.metrics.transfer_entropy_proxy`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adapter-Set Diagnostics\n",
        "\n",
        "For a set of adapters {ΔWᵢ}, compute:\n",
        "- Principal angles between colspans of Aᵢ and Aⱼ.\n",
        "- Subspace overlap cos² mean and effective ranks.\n",
        "\n",
        "These reveal interference vs complementarity. See `plora/la_utils.py` and add summary plots via `scripts/plot_figures.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LoRA Derivations (EYM, Composition, Principal Angles)\n",
        "\n",
        "Let a base parameter matrix be W₀ and an adapter induce a low-rank perturbation ΔW = B A with A ∈ ℝ^{r×n}, B ∈ ℝ^{m×r}.\n",
        "\n",
        "- Eckart–Young–Mirsky (EYM): For any k < rank(ΔW), the best rank-k approximation in Frobenius norm is ΔW_k = U_k S_k V_k^T where U,S,V are from SVD(ΔW). Error: \\(\\|\\Delta W - \\Delta W_k\\|_F^2 = \\sum_{j>k} \\sigma_j^2.\\)\n",
        "- Weighted composition of multiple adapters i with weights w_i:\n",
        "  \\[ W' = W_0 + \\sum_i w_i \\Delta W_i,\\quad \\Delta W_\\mathrm{tot} = \\sum_i w_i \\Delta W_i. \\]\n",
        "  The reprojection error obeys EYM on ΔW_tot.\n",
        "- Stability under small updates: If \\(\\|\\Delta W\\|\\) is small and layer Jacobians have bounded operator norms, the output drift is O(\\|ΔW\\|). A Frobenius trust region enforces \\(\\|ΔW\\|_F \\le \\tau\\).\n",
        "- Composition interference bound: If subspaces span(A_i) are nearly orthogonal (principal angles \\(\\theta_j\\) large), then \\(\\|\\sum_i w_i B_i A_i\\|_F^2 \\approx \\sum_i w_i^2 \\|B_i A_i\\|_F^2\\) up to terms depending on cos(θ_j).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Information Theory (KL/JS, MI, kNN, MINE, TE)\n",
        "\n",
        "- KL and JS: JS(P,Q) = 1/2 KL(P||M) + 1/2 KL(Q||M), M = 1/2(P+Q). Symmetric, finite.\n",
        "- Mutual Information: I(X;Y) = H(X) + H(Y) − H(X,Y). Estimators:\n",
        "  - KSG kNN (continuous) – near-unbiased for moderate N, uses Chebyshev metric.\n",
        "  - MINE (variational lower bound) – train small net with MA correction.\n",
        "- Transfer Entropy: TE(A→B) = H(B_t | B_past) − H(B_t | B_past, A_past). Use discrete histogram or continuous KSG proxy.\n",
        "\n",
        "In-code: `plora/it_estimators.py`, `plora/mine.py`, `plora/te.py`. See tests under `tests/`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graph Diffusion and Mixing-time\n",
        "\n",
        "- Cheeger’s inequalities: \\(\\tfrac{1}{2}\\Phi^2 \\le \\lambda_2 \\le 2\\Phi\\), where Φ is conductance.\n",
        "- Mixing time bound: \\(t_\\text{mix}(\\varepsilon) \\lesssim \\lambda_2^{-1} \\log(1/\\varepsilon\\,\\pi_\\min^{-1})\\).\n",
        "- Push–pull heuristic: \\(t_\\text{diffuse} \\approx C \\log n / \\lambda_2\\). We calibrate C for ER/WS/BA.\n",
        "\n",
        "In-code: `swarm/metrics.py` (spectral gap, conductance), `swarm/theory.py`, `scripts/calibrate_c.py`, `scripts/validate_bounds.py`. Figures via `scripts/plot_figures.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization and Stability (Alternating Train–Merge)\n",
        "\n",
        "- Convergence heuristic: with small ΔW and bounded curvature, alternating local FT and small merges behaves like a damped proximal step.\n",
        "- Trust-region: choose α so that ‖αΔW‖_F ≤ τ, optionally line-search to minimise parameter drift.\n",
        "- Diagnostics: track Frobenius deltas, effective rank, cosine similarity.\n",
        "\n",
        "In-code: `scripts/alternating_train_merge.py` (line search, logs), `plora/loader.py`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Security and Consensus\n",
        "\n",
        "- Signature policy: SHA-256 + RSA-PSS; quorum verification with peer attestation.\n",
        "- Threshold signatures: aggregate multi-sig (RSA fallback) with quorum verification.\n",
        "- Consensus: proposal/vote/commit with safety (one commit per slot), liveness under honest majority.\n",
        "- Audit chain: hash-chained JSONL for tamper detection.\n",
        "- Probes: trigger and clean probes with calibrated thresholds; safetensors per-tensor anomaly z-scores.\n",
        "\n",
        "In-code: `plora/gate.py`, `plora/threshold_sigs.py`, `swarm/consensus.py`, `scripts/audit_verify.py`, `scripts/probes_calibrate.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and common imports\n",
        "import math, json, random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Environment ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 1: SVD reprojection error vs rank (toy)\n",
        "U = np.random.randn(64, 32)\n",
        "V = np.random.randn(32, 64)\n",
        "Delta = U @ V  # rank at most 32\n",
        "s = np.linalg.svd(Delta, compute_uv=False)\n",
        "errs = [np.sum(s[k:]**2) for k in range(1, 33)]\n",
        "plt.plot(range(1,33), errs)\n",
        "plt.xlabel('rank k'); plt.ylabel('Frobenius error^2 beyond k'); plt.title('EYM error curve (toy)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 2: MI vs correlation (KSG estimator)\n",
        "from plora.it_estimators import mi_knn\n",
        "\n",
        "rs = np.linspace(0.0, 0.95, 10)\n",
        "mis = []\n",
        "for rho in rs:\n",
        "    X = np.random.randn(1000, 1)\n",
        "    E = np.random.randn(1000, 1)\n",
        "    Y = rho * X + np.sqrt(1 - rho**2) * E\n",
        "    mis.append(mi_knn(X, Y, k=5))\n",
        "plt.plot(rs, mis, marker='o')\n",
        "plt.xlabel('rho'); plt.ylabel('MI (nats)'); plt.title('KSG MI vs correlation')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 3: MINE MI on Gaussian (short run)\n",
        "import torch\n",
        "from plora.mine import mine_estimate, MineConfig\n",
        "\n",
        "X = torch.randn(2000, 2)\n",
        "rho = 0.8\n",
        "Y = rho * X + (1 - rho**2)**0.5 * torch.randn_like(X)\n",
        "mi, _ = mine_estimate(X, Y, cfg=MineConfig(epochs=100, batch_size=256))\n",
        "print(f\"MINE MI ~ {mi:.3f} nats at rho={rho}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 4: TE directionality on AR process\n",
        "from plora.te import transfer_entropy_discrete\n",
        "\n",
        "def ar_process(n=600, rho=0.8):\n",
        "    a = np.random.randn(n)\n",
        "    b = np.zeros(n)\n",
        "    for t in range(1, n):\n",
        "        b[t] = rho * a[t-1] + (1-rho) * b[t-1] + 0.1 * np.random.randn()\n",
        "    return a, b\n",
        "\n",
        "a, b = ar_process()\n",
        "te_ab = transfer_entropy_discrete(a, b, k=1, bins=8)\n",
        "te_ba = transfer_entropy_discrete(b, a, k=1, bins=8)\n",
        "print(f\"TE A->B: {te_ab:.3f}, TE B->A: {te_ba:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 5: Diffusion time vs spectral gap (toy ER)\n",
        "from swarm.graph_v2 import erdos_renyi_graph\n",
        "from swarm.metrics import spectral_gap\n",
        "\n",
        "ns = [20, 30, 40, 50]\n",
        "lam2s = []\n",
        "for n in ns:\n",
        "    g = erdos_renyi_graph(n, 0.25, seed=42+n)\n",
        "    lam2s.append(spectral_gap(g))\n",
        "plt.plot(ns, lam2s, marker='o')\n",
        "plt.xlabel('n'); plt.ylabel('lambda2'); plt.title('Spectral gap vs n (ER p=0.25)')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
